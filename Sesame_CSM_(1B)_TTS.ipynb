{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25GZe12bi7I8"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
    "</div>\n",
    "\n",
    "Read our **[TTS Guide](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** for instructions and all our notebooks.\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClNui48Hi7JH"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_bMkv7ki7JL"
   },
   "source": [
    "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ad-kbALi7JN"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Puj19m7qi7JP"
   },
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "\n",
    "# Must install latest transformers for Sesame!\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-LqUgufi7JU"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_R0oWSwi7JV",
    "outputId": "224dee66-de62-49a3-d821-a2cb87936a46"
   },
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import CsmForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model, processor = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/csm-1b\",\n",
    "    max_seq_length= 2048, # Choose any for long context!\n",
    "    dtype = None, # Leave as None for auto-detection\n",
    "    auto_model = CsmForConditionalGeneration,\n",
    "    load_in_4bit = False, # Select True for 4bit - reduces memory usage\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPTVeI9ni7JY"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dk5azmwi7Jb",
    "outputId": "05e85f01-d45f-41cb-c155-466074768139"
   },
   "source": [
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cflyBsb9i7Je"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep  \n",
    "\n",
    "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QeCRvPI5i7Jh"
   },
   "source": [
    "#@title Dataset Prep functions\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "processor = AutoProcessor.from_pretrained(\"unsloth/csm-1b\")\n",
    "\n",
    "# Load dataset\n",
    "raw_ds = load_dataset(\"taresh18/AnimeVox\", split=\"train[:1250]\")\n",
    "if \"source\" not in raw_ds.column_names:\n",
    "    new_column = [\"0\"] * len(raw_ds)\n",
    "    raw_ds = raw_ds.add_column(\"source\", new_column)\n",
    "\n",
    "target_sampling_rate = 24000\n",
    "raw_ds = raw_ds.cast_column(\"audio\", Audio(sampling_rate=target_sampling_rate))\n",
    "\n",
    "# Add audio length check\n",
    "def check_audio_length(example):\n",
    "    return {\"audio_length\": len(example[\"audio\"][\"array\"])}\n",
    "\n",
    "raw_ds = raw_ds.map(check_audio_length)\n",
    "\n",
    "# Filter out long audio files\n",
    "max_audio_length = 240000  # Slightly less than 240001 to be safe\n",
    "filtered_raw_ds = raw_ds.filter(lambda x: x[\"audio_length\"] <= max_audio_length)\n",
    "print(f\"Filtered out {len(raw_ds) - len(filtered_raw_ds)} examples out of {len(raw_ds)} total\")\n",
    "\n",
    "# Keep original preprocessing function as close as possible\n",
    "def preprocess_example(example):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": str(example[\"source\"]),\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example[\"transcription\"]},\n",
    "                {\"type\": \"audio\", \"path\": example[\"audio\"][\"array\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        model_inputs = processor.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            output_labels=True,\n",
    "            text_kwargs = {\n",
    "                \"padding\": \"max_length\", # pad to the max_length\n",
    "                \"max_length\": 250, # this should be the max length of audio\n",
    "                \"pad_to_multiple_of\": 8,\n",
    "                \"padding_side\": \"right\",\n",
    "            },\n",
    "            audio_kwargs = {\n",
    "                \"sampling_rate\": 24_000,\n",
    "                \"max_length\": 240001, # max input_values length of the whole dataset\n",
    "                \"padding\": \"max_length\",\n",
    "            },\n",
    "            common_kwargs = {\"return_tensors\": \"pt\"},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example with transcription '{example['transcription'][:50]}...': {e}\")\n",
    "        return None\n",
    "\n",
    "    required_keys = [\"input_ids\", \"attention_mask\", \"labels\", \"input_values\", \"input_values_cutoffs\"]\n",
    "    processed_example = {}\n",
    "    # print(model_inputs.keys())\n",
    "    for key in required_keys:\n",
    "        if key not in model_inputs:\n",
    "            print(f\"Warning: Required key '{key}' not found in processor output for example.\")\n",
    "            return None\n",
    "\n",
    "        value = model_inputs[key][0]\n",
    "        processed_example[key] = value\n",
    "\n",
    "\n",
    "    # Final check (optional but good)\n",
    "    if not all(isinstance(processed_example[key], torch.Tensor) for key in processed_example):\n",
    "         print(f\"Error: Not all required keys are tensors in final processed example. Keys: {list(processed_example.keys())}\")\n",
    "         return None\n",
    "\n",
    "    return processed_example\n",
    "\n",
    "# Process the dataset\n",
    "processed_ds = filtered_raw_ds.map(\n",
    "    preprocess_example,\n",
    "    remove_columns=[col for col in filtered_raw_ds.column_names],\n",
    "    desc=\"Preprocessing dataset\",\n",
    ")\n",
    "\n",
    "# Filter out None values (failed preprocessing)\n",
    "valid_indices = [i for i, example in enumerate(processed_ds) if example is not None]\n",
    "processed_ds = processed_ds.select(valid_indices)\n",
    "print(f\"Removed {len(filtered_raw_ds) - len(processed_ds)} examples that failed preprocessing\")\n",
    "\n",
    "# Split into train and validation (5%)\n",
    "train_idx, eval_idx = train_test_split(\n",
    "    range(len(processed_ds)),\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_ds = processed_ds.select(train_idx)\n",
    "eval_ds = processed_ds.select(eval_idx)\n",
    "\n",
    "print(f\"Training set size: {len(train_ds)}\")\n",
    "print(f\"Validation set size: {len(eval_ds)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTIxxQqai7Jj"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G03LvbfOi7Jm"
   },
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = processed_ds,\n",
    "     eval_dataset  = eval_ds,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_ratio = 0.08,\n",
    "        num_train_epochs = 14.5,\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        # weight_decay = 0.01, # Turn this on if overfitting\n",
    "     lr_scheduler_type   = \"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs = {\"min_lr_rate\": 0.10},   # floor = 2e-5\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        # report_to = \"wandb\", # Use this for WandB etc\n",
    "\n",
    "        # Checkpoint saving parameters\n",
    "        save_strategy = \"steps\",     # Save based on steps, not epochs\n",
    "        save_steps = \t10,\n",
    "         eval_strategy               = \"steps\",\n",
    "         eval_steps                  = \t10,\n",
    "        # Save every 50 steps\n",
    "        metric_for_best_model       = \"eval_loss\",\n",
    "        save_total_limit = 10,        # Keep only the 3 most recent checkpoints\n",
    "        load_best_model_at_end      = True,\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pEAe-cLi7Jo",
    "outputId": "e847fddd-977d-478e-af4f-93c806a370ef"
   },
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()  # This will provide a URL and key to authenticate\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "szZOUYHgi7Js",
    "outputId": "99928ed1-f42c-4d6f-b296-56e6f5923383"
   },
   "source": "trainer_stats = trainer.train(resume_from_checkpoint = True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjWdtI8zi7Jv",
    "outputId": "f08ea38e-de70-47bc-f517-61ee00f18c6b"
   },
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWCCkIXyi7Jy"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the prompts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "LwAagCSwi7J0",
    "outputId": "1cfd99f6-2dbb-406a-b1d1-b693f6c8b795"
   },
   "source": [
    "from IPython.display import Audio, display\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# ── 1.  Your five evaluation sentences ───────────────────────────────────\n",
    "test_lines = [\n",
    "    \"Please remind me to send the invoice before noon, then check if the client confirmed the lunch reservation at the corner cafe for tomorrow afternoon.\",\n",
    "    \"I forgot my umbrella on the train again this morning, which means either I get soaked later or the weather app is finally wrong for once.\",\n",
    "    \"The coffee machine keeps flashing some strange error code, yet somehow still manages to make a better espresso than any cafe near the office.\",\n",
    "    \"If the meeting actually starts on time and doesn't spiral into a group therapy session, I’ll order pizza for everyone and pretend I believe in structure.\",\n",
    "    \"Traffic was unusually light this morning, so for a solid five minutes I genuinely believed I had forgotten a national holiday or time itself was broken.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "speaker_id = 0              # pick your speaker token if you have multiples\n",
    "sample_rate = 24_000        # CSM default\n",
    "\n",
    "# ── 2.  Generate, save, and play each line ───────────────────────────────\n",
    "for idx, text in enumerate(test_lines, start=1):\n",
    "    prompt = f\"[{speaker_id}]{text}\"\n",
    "    inputs  = processor(prompt, add_special_tokens=True).to(\"cuda\")\n",
    "\n",
    "    audio_values = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 275,        # enough for ~10 s of audio\n",
    "        output_audio   = True,\n",
    "    )\n",
    "\n",
    "    audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
    "\n",
    "    fname = f\"test_line_{idx:02d}.wav\"\n",
    "    sf.write(fname, audio, sample_rate)\n",
    "    print(f\"Saved → {fname}\")\n",
    "\n",
    "    display(Audio(audio, rate=sample_rate))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "5a5Dv6gzXa_p",
    "outputId": "fe7d8890-a099-4000-819e-f5813eb4d307"
   },
   "source": [
    "text = \"Chayce is a faggot dayum\"\n",
    "\n",
    "speaker_id = 1\n",
    "# Another equivalent way to prepare the inputs\n",
    "conversation = [\n",
    "    {\"role\": str(speaker_id), \"content\": [{\"type\": \"text\", \"text\": text}]},\n",
    "]\n",
    "audio_values = model.generate(\n",
    "    **processor.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "    ).to(\"cuda\"),\n",
    "    max_new_tokens=125, # 125 tokens is 10 seconds of audio, for longer text increase this\n",
    "    output_audio=True\n",
    ")\n",
    "audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
    "sf.write(\"example_without_context.wav\", audio, 24000)\n",
    "display(Audio(audio, rate=24000))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJIc7oYdi7J2"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfyvxgUEi7J3",
    "outputId": "6296c5b1-f7fc-47ba-dca5-77f74d89914c"
   },
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "processor.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# processor.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eve_aZkli7J9"
   },
   "source": [
    "### Saving to float16\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-bAEpXHHi7J-"
   },
   "source": [
    "# Merge to 16bit\n",
    "if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8gOHT6li7KB"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
